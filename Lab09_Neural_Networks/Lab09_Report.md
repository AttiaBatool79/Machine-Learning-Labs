
# Lab 09 - Neural Networks

## Table of Contents
- [What I Did](#what-i-did)
- [What is a Neural Network?](#what-is-a-neural-network)
- [Tools I Used](#tools-i-used)
- [Steps I Followed](#steps-i-followed)

---

## What I Did
In this lab, I learned how to build a simple neural network using Python. I studied how data moves through the network (forward propagation) and how the network learns by updating itself (backward propagation). I also explored how activation functions work.

---

## What is a Neural Network?
A neural network is a computer system inspired by the human brain. It helps machines understand and make decisions based on data. It takes inputs, processes them through layers, and gives outputs. Each layer uses functions to decide what to do with the data.

---

## Tools I Used
- Python
- NumPy (for mathematical operations)
- Matplotlib (for plotting graphs)

---

## Steps I Followed

### 1. Activation Functions
These are special math functions that help the network learn. Common ones are:
- **Sigmoid**: Converts values between 0 and 1.
- **ReLU (Rectified Linear Unit)**: Turns all negative values to zero, keeps positive ones as they are.

### 2. Matrix Initialization
I started by setting up input values, weights, and biases. This is the first step before training the model.

### 3. Forward Propagation
In this step, data goes from input to output through the network. Each layer calculates and passes its result to the next layer.

### 4. Backward Propagation
This is the learning step. The network looks at the error (difference between expected and actual output), and updates the weights and biases to reduce the error.

### 5. Training and Testing
I trained the model by repeating forward and backward propagation many times. After training, I tested it to see how well it learned.

---

## Summary
This lab helped me understand the basics of how neural networks work and how they learn. I practiced writing simple code to build and train the network, and now I have a better idea of how machine learning works.


